{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_id=\"psychAiD\"\n",
    "\n",
    "# set shot num\n",
    "n_shot=1\n",
    "\n",
    "\n",
    "# Clinical\n",
    "test_path='data/psych/task4.jsonl'\n",
    "val_path='data/psych/task4.jsonl'\n",
    "\n",
    "\n",
    "# save path\n",
    "output_dir='data/fewshot1_task4'\n",
    "\n",
    "os.system('python -u ./src/generate_fewshot_psych.py \\\n",
    "    --n_shot={} \\\n",
    "    --model_id={} \\\n",
    "    --output_dir={}  \\\n",
    "    --val_path={} \\\n",
    "    --test_path={} > output.log 2>&1'.format(n_shot,model_id,output_dir,val_path,test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### QC\n",
    "import json\n",
    "\n",
    "nshot = 0\n",
    "for task in ['1','2','3_1','4','5']:\n",
    "\n",
    "\n",
    "    print('-'*25,task,'-'*25)\n",
    "    for model in ['gpt-3.5-turbo','gpt-4o-mini','gpt-4','gemini-1.5-pro','glm4','hunyuan-lite','hunyuan-pro','minimax','spark-4ultra','baichuan4','deepseek','doubao-pro-32k','ernie-4-8k','moonshot-v1-32k','yi-large','qwen-max']: \n",
    "    \n",
    "        if task == '5': nshot=0\n",
    "        output_pth = './result/API/{}shot/task{}_{}.json'.format(nshot,task,model)\n",
    "        lines = []\n",
    "        id = 1\n",
    "        total = 0\n",
    "        failed = []\n",
    "        try:\n",
    "            with open(output_pth, 'r', encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    total += 1\n",
    "                    data = json.loads(line)\n",
    "                    answer_key = [key for key in data.keys() if 'answer_' in key][0]\n",
    "                    if data[answer_key] == 'API call failed':\n",
    "                        failed.append(id)\n",
    "                    id += 1\n",
    "            print(model,'\\t',total,'\\t',len(failed),'\\t',failed)\n",
    "        except:\n",
    "            print(output_pth,'not exist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### reformat json\n",
    "import json\n",
    "\n",
    "nshot = 0\n",
    "\n",
    "for task in ['1','2','3','4','5']:\n",
    "    for model in ['gpt-3.5-turbo','gpt-4o-mini','gpt-4','gemini-1.5-pro','glm4','hunyuan-lite','hunyuan-pro','minimax','spark-4ultra','baichuan4','deepseek','doubao-pro-32k','ernie-4-8k','moonshot-v1-32k','yi-large','qwen-max']: \n",
    "\n",
    "        if task == '5': nshot = 0\n",
    "        output_pth = './result/API/{}shot/task{}_{}.json'.format(nshot,task,model)\n",
    "        lines = []\n",
    "        try:\n",
    "            with open(output_pth, 'r', encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    lines.append(json.loads(line))\n",
    "        except:\n",
    "            print(output_pth,'not exist')\n",
    "            continue\n",
    "        with open(output_pth.replace('result','result-refined').replace('task3_1','task3'), 'w', encoding=\"utf-8\") as f:\n",
    "            json.dump(lines, f, ensure_ascii=False, indent=4)\n",
    "        output_pth = output_pth.replace('result','result-refined').replace('task3_1','task3')\n",
    "        print(f'output to {output_pth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "nshot = 1\n",
    "for task in [1,3]:\n",
    "    print('-'*50,'task{}'.format(task),'-'*50)\n",
    "\n",
    "    for model in ['gpt-3.5-turbo','gpt-4o-mini','gpt-4','gemini-1.5-pro','glm4','hunyuan-lite','hunyuan-pro','minimax','spark-4ultra','baichuan4','deepseek','doubao-pro-32k','ernie-4-8k','moonshot-v1-32k','yi-large','qwen-max']: \n",
    "        \n",
    "        print(model)\n",
    "        ans_path = './result-refined/API/{}shot/task{}_{}.json'.format(nshot,task,model)\n",
    "        dir_out = './result/PsychClinical/{}shot/{}_api/task{}'.format(nshot,model,task)\n",
    "\n",
    "        if not os.path.exists(ans_path):\n",
    "            print('Error:',ans_path,'not exist!!!')\n",
    "            continue\n",
    "\n",
    "        os.system('python -u ./src/calc_metrics.py \\\n",
    "            --ans_path={} \\\n",
    "            --dir_out={} \\\n",
    "            > output_calc.log 2>&1'.format(ans_path,dir_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### calc metrics\n",
    "import tqdm\n",
    "import numpy as np \n",
    "import os\n",
    "import json \n",
    "\n",
    "## 完整性\n",
    "def compute_integrity(ans):\n",
    "    cnt = 0\n",
    "    if '病程标准' in ans: cnt += 1\n",
    "    if '症状学标准' in ans: cnt += 1\n",
    "    if '严重程度标准' in ans: cnt += 1\n",
    "    if '排除标准' in ans: cnt += 1\n",
    "    \n",
    "    return cnt/4*100\n",
    "\n",
    "## 准确性\n",
    "import re\n",
    "def find_time(text):\n",
    "    # 使用正则表达式匹配时间描述\n",
    "    time_patterns = [\n",
    "        r'\\d+年',\n",
    "        r'\\d+月',\n",
    "        r'\\d+天',\n",
    "        # r'\\d+余',\n",
    "        r'\\d+余日',\n",
    "        r'\\d+余月',\n",
    "        r'\\d+余年'\n",
    "    ]\n",
    "\n",
    "    times_found = []\n",
    "    for pattern in time_patterns:\n",
    "        times_found.extend(re.findall(pattern, text))\n",
    "\n",
    "    # 去重并排序结果\n",
    "    unique_times = sorted(set(times_found))\n",
    "    return unique_times\n",
    "\n",
    "def find_des(text):\n",
    "    # 使用正则表达式匹配时间描述\n",
    "    des_patterns = [\n",
    "        r'慢性',\n",
    "        r'急性',\n",
    "        r'持续',\n",
    "        r'连续',\n",
    "        r'间断',\n",
    "        r'亚急性',\n",
    "        r'反复',\n",
    "        r'波动',\n",
    "    ]\n",
    "\n",
    "    des_found = []\n",
    "    for pattern in des_patterns:\n",
    "        des_found.extend(re.findall(pattern, text))\n",
    "\n",
    "    # 去重并排序结果\n",
    "    unique_times = sorted(set(des_found))\n",
    "    if '急性' in unique_times and '亚急性' in unique_times:\n",
    "        unique_times = [a for a in unique_times if a != '急性']\n",
    "    \n",
    "    return unique_times\n",
    "\n",
    "import re\n",
    "def find_icd(text):\n",
    "    # text = \"患者的主要诊断是 F30.901，需要进行进一步的治疗。\"\n",
    "    icd_dict = {}\n",
    "    with open('./data/psych/icd-10.jsonl','r',encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            data = json.loads(line)\n",
    "            icd_dict[data[\"ICD-10 Name\"]] = data['ICD-10 Code']\n",
    "    icd10_codes = re.findall(r'\\bF\\d+\\.\\d+\\b', text)\n",
    "    if len(icd10_codes) < 1:\n",
    "        for name in icd_dict.keys():\n",
    "            if name in text:\n",
    "                return icd_dict[name]\n",
    "        # print('failed to find icd in:',text)\n",
    "        return ''\n",
    "    return icd10_codes[0]  # 输出: ['F30.901']\n",
    "\n",
    "def acc_task1(out,tgt):\n",
    "    try:\n",
    "        out_zs = out.split('主诉')[1].split('病程标准')[0].split('病例特点')[0].replace('：','').replace(':','').replace('\\n','')\n",
    "    except:\n",
    "        out_zs = out.split('病程标准')[0]\n",
    "        # print(out)\n",
    "    tgt_zs = tgt.split('主诉：')[1].split('病例特点')[0].replace('\\n','')\n",
    "    \n",
    "    hit = 0\n",
    "    tgt_num = 0\n",
    "    \n",
    "    times_zs = find_time(tgt_zs)\n",
    "    tgt_num += len(times_zs)\n",
    "    for t in times_zs:\n",
    "        if t in out_zs:\n",
    "            hit += 1\n",
    "\n",
    "    if '病程标准' in out:\n",
    "        out_bc = out.split('病程标准')[1].split('症状学标准')[0].replace('：','').replace(':','').replace('\\n','').replace('*','')\n",
    "    else:\n",
    "        out_bc = ''\n",
    "    tgt_bc = tgt.split('病程标准：')[1].split('2.')[0].replace('\\n','')\n",
    "    key_bc =  find_time(tgt_bc) + find_des(tgt_bc)\n",
    "    tgt_num += len(key_bc)\n",
    "    for k in key_bc:\n",
    "        if k in out_bc:\n",
    "            hit += 1\n",
    "    \n",
    "    if '严重程度标准' in out:\n",
    "        out_yz = out.split('严重程度标准')[1].split('排除标准')[0].replace('：','').replace(':','').replace('\\n','').replace('*','')\n",
    "    else:\n",
    "        out_yz = ''\n",
    "    tgt_yz = tgt.split('严重程度标准：')[1].split('4.')[0].replace('\\n','')\n",
    "    opt = ['严重','明显','轻微']\n",
    "    key_yz =  [o for o in opt if o in tgt_yz]\n",
    "    tgt_num += len(key_yz)\n",
    "    for k in key_yz:\n",
    "        if k in out_yz:\n",
    "            hit += 1\n",
    "\n",
    "    try:\n",
    "        acc = hit / tgt_num\n",
    "    except:\n",
    "        acc = 0\n",
    "        print(tgt)\n",
    "    return acc*100\n",
    "\n",
    "def acc_task2(out,tgt):\n",
    "    out_icd = find_icd(out.split('精神科共病诊断：')[0])\n",
    "    icd_dict = {}\n",
    "    with open('./data/psych/icd-10.jsonl','r',encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            data = json.loads(line)\n",
    "            icd_dict[data[\"ICD-10 Name\"]] = data['ICD-10 Code']\n",
    "    tgt_icd = icd_dict[tgt.split('主要诊断：')[1].split('精神科共病诊断：')[0].replace(' ','').replace('\\n','')]\n",
    "    \n",
    "    # print(out_icd,tgt_icd)\n",
    "    acc = 0\n",
    "    if out_icd[:3] == tgt_icd[:3]:\n",
    "        acc = 0.5\n",
    "    if out_icd == tgt_icd:\n",
    "        acc = 1\n",
    "    return acc*100\n",
    "\n",
    "\n",
    "def refine_name(text):\n",
    "    names = ['人格障碍',\n",
    "            '双相情感障碍',\n",
    "            '器质性精神障碍',\n",
    "            '复发性抑郁障碍',\n",
    "            '妄想性障碍',\n",
    "            '广泛性焦虑障碍',\n",
    "            '强迫性障碍',\n",
    "            '心境障碍',\n",
    "            '急性而短暂的精神病性障碍',\n",
    "            '抑郁发作',\n",
    "            '抑郁障碍',\n",
    "            '焦虑障碍',\n",
    "            '环性心境',\n",
    "            '精神分裂症',\n",
    "            '精神障碍',\n",
    "            '脑器质性精神病',\n",
    "            '躁狂发作',\n",
    "            '躯体症状障碍',\n",
    "            '酒精所致的精神行为障碍',\n",
    "            '阿尔兹海默',\n",
    "            '阿尔茨海默']\n",
    "    for name in names:\n",
    "        if name in text:\n",
    "            return name\n",
    "    if len(re.findall(r'\\“(.*?)\\”',text)) > 0:\n",
    "        return re.findall(r'\\“(.*?)\\”',text)[0]\n",
    "    return text\n",
    "\n",
    "def acc_task3_1(out,tgt):\n",
    "    \n",
    "    out = out.replace('：',':').replace(',','，')\n",
    "    try:\n",
    "        out_list = re.findall(r'\\[(.*?)\\]', out)\n",
    "        out_list = [out for out in out_list if out!= '疾病名称']\n",
    "        out_main = out_list[0]\n",
    "        out_dif1 = out_list[1]\n",
    "        out_dif2 = out_list[2]\n",
    "\n",
    "    except:\n",
    "        try:\n",
    "            out_main = out.split('主要诊断')[1].split('\\n')[0].replace('[','').replace(']','').replace(':','')\n",
    "            out_dif1 = out.split('<鉴别诊断1>')[1].split('\\n')[0].split('，')[0].replace('[','').replace(']','').replace(':','')\n",
    "            out_dif2 = out.split('<鉴别诊断2>')[1].split('\\n')[0].split('，')[0].replace('[','').replace(']','').replace(':','')\n",
    "        except:\n",
    "            try:\n",
    "                out_main_part = out.split('鉴别诊断:')[0]\n",
    "                out_main = refine_name(out_main_part)\n",
    "                if out_main == out_main_part:\n",
    "                    out_main = ''\n",
    "            except:\n",
    "                out_main = ''\n",
    "            try:\n",
    "                out_dif1 = out.split('<鉴别诊断1>')[1].split('<鉴别诊断2>')[0].split('鉴别点:')[0].replace('，','')\n",
    "            except:\n",
    "                \n",
    "                try:\n",
    "                    out_dif1 = out.split('鉴别诊断:')[1].split(':')[0].split('，')[0].replace('，','')\n",
    "                except:\n",
    "                    try:\n",
    "                        if '-' in out.split('鉴别诊断')[-1]:\n",
    "                            out_dif1 = out.split('鉴别诊断')[-1].split('-')\n",
    "                            if len(out_dif1) >= 3:\n",
    "                                out_dif1 = out_dif1[-2].split(':')[0].split('，')[0].replace('，','')\n",
    "                            else:\n",
    "                                out_dif1 = out_dif1[1].split(':')[0].split('，')[0].replace('，','')\n",
    "                        elif '1' in out.split('鉴别诊断')[-1]:\n",
    "                            out_dif1 = out.split('鉴别诊断')[-1].split('1')[-1].split(':')[0].split('，')[0].replace('，','')\n",
    "                        else:\n",
    "                            out_dif1 = out.split('鉴别诊断')[-1].split('，')[0].split(':')[-1]\n",
    "                    except:\n",
    "                        print(out)\n",
    "                        out_dif1 = ''\n",
    "            try:\n",
    "                out_dif2 = out.split('<鉴别诊断2>')[1].split('鉴别点:')[0].replace('，','')\n",
    "            except:\n",
    "                # print(out)\n",
    "                try:\n",
    "                    out_dif2 = out.split('鉴别诊断:')[1].split('2')[-1].split('-')[-1].split(':')[0].split('，')[0].replace('，','')\n",
    "                except:\n",
    "                    try:\n",
    "                        if '-' in out.split('鉴别诊断')[-1]:\n",
    "                            out_dif2 = out.split('鉴别诊断')[-1].split('-')\n",
    "                            if len(out_dif1) >= 3:\n",
    "                                out_dif2 = out_dif2[-1].split(':')[0].split('，')[0].replace('，','')\n",
    "                            else:\n",
    "                                out_dif2 = ''\n",
    "                        elif '2' in out.split('鉴别诊断')[-1]:\n",
    "                            out_dif2 = out.split('鉴别诊断')[-1].split('2')[-1].split(':')[0].split('，')[0].replace('，','')\n",
    "                        else:\n",
    "                            out_dif2 = out.split('鉴别诊断')[-1].replace('\\n\\n','\\n').split('\\n')[-1].split('，')[0].split(':')[-1]\n",
    "                    except:\n",
    "                        print(out)\n",
    "                        out_dif2 = ''\n",
    "\n",
    "    out_main = refine_name(out_main)\n",
    "    out_dif1 = refine_name(out_dif1)\n",
    "    out_dif2 = refine_name(out_dif2) \n",
    "    if fuzzy_match(out_main,tgt['main']):\n",
    "        acc_main = 1\n",
    "    else:\n",
    "        acc_main = 0\n",
    "    \n",
    "    acc_diff = 0\n",
    "    if fuzzy_match_list(out_dif1,tgt['diff']):\n",
    "        acc_diff += 0.5\n",
    "    if fuzzy_match_list(out_dif2,tgt['diff']):\n",
    "        acc_diff += 0.5\n",
    "    # print(out_main,'|',out_dif1,'|',out_dif2)\n",
    "    return acc_main*100,acc_diff*100\n",
    "\n",
    "\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def fuzzy_match(str1, str2):\n",
    "    \n",
    "    ratio = fuzz.ratio(str1, str2)\n",
    "    if ratio > 50:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def fuzzy_match_list(str1, tgt_list):\n",
    "    \n",
    "    flag = False\n",
    "    for str2 in tgt_list:\n",
    "        if fuzzy_match(str1,str2):\n",
    "            flag = True\n",
    "            break\n",
    "    return flag\n",
    "\n",
    "def hit_task4(out,tgt):\n",
    "    drugs = ['丙戊酸','伏硫西汀','利培酮','劳拉西泮','唑吡坦','喹硫平','地西泮','奋乃静','奥氮平','奥沙西泮','帕利哌酮','帕罗西汀','度洛西汀','拉莫三嗪','文拉法辛','曲唑酮','氟伏沙明','氟哌啶醇','氟西汀','氨磺必利','氯丙嗪','氯氮平','硝西泮','碳酸锂','米氮平','美金刚','舍曲林','艾司西酞普兰','阿戈美拉汀','阿立哌唑','鲁拉西酮','齐拉西酮']\n",
    "    out = out.replace('，',',').replace(' ','').replace('\\n\\n','\\n').replace('：',':')\n",
    "    tgt = tgt.replace('，',',').replace(' ','')\n",
    "    try:\n",
    "        if '推荐药物' in out:\n",
    "            out_chunks = '推荐药物'.join(out.split('推荐药物')[1:])\n",
    "        else:\n",
    "            out_chunks = out.split('\\n')\n",
    "            if len(out_chunks) == 1:\n",
    "                out_chunks = out_chunks\n",
    "            elif '分析' in out_chunks[0]:\n",
    "                out_chunks = out_chunks[1:]\n",
    "            # out_chunks = out.split('\\n')[1:]\n",
    "        \n",
    "        if isinstance(out_chunks,str):\n",
    "            out_chunks = [out_chunks]\n",
    "        ans_idx = 0\n",
    "        max_match = 0\n",
    "        for idx,piece in enumerate(out_chunks):\n",
    "            match = 0\n",
    "            for drug in drugs:\n",
    "                if drug in piece: match += 1\n",
    "            if match>max_match:\n",
    "                max_match = match\n",
    "                ans_idx = idx\n",
    "\n",
    "        # out_list = out_chunks[ans_idx].split(':')[-1].replace('。','') #.split(',')#.split('、')\n",
    "        out_list = []\n",
    "        for drug in drugs:\n",
    "            if drug in out_chunks[ans_idx]:\n",
    "                out_list.append(drug)\n",
    "        out_list.sort(key=lambda x: out_chunks[ans_idx].find(x))\n",
    "    except:\n",
    "    #     out_list = out.split('分析')[0].split(',').split(' ')\n",
    "    \n",
    "    # if len(out_list[0]) > 6:\n",
    "        out_list = []\n",
    "        for drug in drugs:\n",
    "            if drug in '\\n'.join(out.split('推荐药物')[1:]):\n",
    "                out_list.append(drug)\n",
    "        out_list.sort(key=lambda x: ('\\n'.join(out.split('推荐药物')[1:])).find(x))\n",
    "    \n",
    "    if len(out_list) ==0:\n",
    "        out_list = []\n",
    "        for drug in drugs:\n",
    "            if drug in '\\n'.join(out.split('\\n')[1:]):\n",
    "                out_list.append(drug)\n",
    "        out_list.sort(key=lambda x: ('\\n'.join(out.split('\\n')[1:])).find(x))\n",
    "        \n",
    "    if len(out_list) ==0:  \n",
    "        out_list = ['']\n",
    "        # print('failed to match drug:',out)\n",
    "\n",
    "    \n",
    "    tgt_list = tgt.split(',')\n",
    "    print(out_list,'|',tgt_list)\n",
    "    \n",
    "    o_in_t = [d for d in out_list if d in tgt_list]\n",
    "    t_in_o = [d for d in tgt_list if d in out_list]\n",
    "    acc = 0\n",
    "    if out_list[0]==tgt_list[0]:\n",
    "        acc = 1\n",
    "    recall = len(t_in_o) / len(tgt_list)\n",
    "    precision = len(o_in_t) / len(out_list)\n",
    "    \n",
    "    return acc*100,precision*100,recall*100\n",
    "\n",
    "\n",
    "def task5_exam_acc(out,tgt):\n",
    "    correct = 0\n",
    "    ans = re.findall(r'[A-Z]\\.', out)\n",
    "    if len(ans)<len(tgt):\n",
    "        ans = re.findall(r'[A-Z]', out)\n",
    "        if len(ans)<len(tgt):\n",
    "            # print(out)\n",
    "            # print('-'*50)\n",
    "            if len(ans) < 1:\n",
    "                return 0\n",
    "            else:\n",
    "                for a,t in zip(ans,tgt[:len(ans)]):\n",
    "                    if a==t:\n",
    "                        correct += 1\n",
    "                return correct/len(tgt)\n",
    "    else:\n",
    "        ans = [a.replace('.','') for a in ans]\n",
    "\n",
    "    ans = ans[-len(tgt):]\n",
    "    \n",
    "    for a,t in zip(ans,tgt):\n",
    "        if a==t:\n",
    "            correct += 1\n",
    "    return correct/len(tgt)\n",
    "\n",
    "\n",
    "\n",
    "### calc\n",
    "\n",
    "nshot = 0\n",
    "metrics_list = []   \n",
    "task3_ans = np.load('./data/psych/task3_ans.npy',allow_pickle=True).item()\n",
    "  \n",
    "for task in [1,2,3,4]:\n",
    "\n",
    "    print('-'*100)\n",
    "    print('task',task)\n",
    "    for model in ['gpt-3.5-turbo','gpt-4o-mini','gpt-4','gemini-1.5-pro','glm4','hunyuan-lite','hunyuan-pro','minimax','spark-4ultra','baichuan4','deepseek','doubao-pro-32k','ernie-4-8k','moonshot-v1-32k','yi-large','qwen-max']: \n",
    "        \n",
    "        ans_path = './result-refined/API/{}shot/task{}_{}.json'.format(nshot,task,model)\n",
    "        if not os.path.exists(ans_path):\n",
    "            print(ans_path,'not exist')\n",
    "            continue\n",
    "        # load data\n",
    "        lst_tgt = []\n",
    "        lst_out = []\n",
    "        lst_idx = []\n",
    "        lst_ipid = []\n",
    "        option_qa = [[],[]]\n",
    "        with open(ans_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            answers = json.load(f)\n",
    "        idx = 0\n",
    "        for ans in answers:\n",
    "            if 'question_type' not in ans.keys():\n",
    "                ans['question_type'] = 'clinical'\n",
    "            if '选择题' in ans['question_type']:\n",
    "                option_qa[0].append(ans['answer'])\n",
    "                option_qa[1].append(ans['model_answer'])\n",
    "                idx += 1\n",
    "            else:\n",
    "                if ans['question_type'] == 'clinical':\n",
    "                    if task == '5_exam':\n",
    "                        lst_tgt.append(ans['answer'])\n",
    "                    elif ans['conversations'][1]['from']=='gpt':\n",
    "                        lst_tgt.append(ans['conversations'][1]['value'])\n",
    "                    else:\n",
    "                        lst_tgt.append(ans['conversations'][2]['value'])\n",
    "                else:\n",
    "                    lst_tgt.append(ans['answer'])\n",
    "                # lst_out.append(ans['model_answer'])\n",
    "                lst_out.append(ans['answer_0'])\n",
    "                lst_ipid.append(ans['id'])\n",
    "                lst_idx.append(idx)\n",
    "                idx += 1\n",
    "\n",
    "        # compute scores of each sample across entire dataset\n",
    "        scores_all = {}\n",
    "        # for tgt, out, idx in tqdm.tqdm(zip(lst_tgt, lst_out, lst_idx)):\n",
    "        for tgt, out, idx, ipid in zip(lst_tgt, lst_out, lst_idx,lst_ipid):\n",
    "\n",
    "            if out == 'API调用失败':\n",
    "                continue\n",
    "            # get sub-dict containing scores for each metric\n",
    "            # task1\n",
    "            if 'task1' in ans_path:\n",
    "                metrics = ['integrity','Acc']\n",
    "                scores = [compute_integrity(out),acc_task1(out,tgt)]\n",
    "            \n",
    "            # task2\n",
    "            if 'task2' in ans_path:\n",
    "                metrics = ['Acc']\n",
    "                scores = acc_task2(out,tgt)\n",
    "            \n",
    "            # task3\n",
    "            if 'task3' in ans_path:\n",
    "                metrics = ['Acc_main','Acc_diff']\n",
    "                tgt = task3_ans[ipid]\n",
    "                # scores = acc_task3(out,tgt)\n",
    "                try:\n",
    "                    scores = acc_task3_1(out,tgt)\n",
    "                except:\n",
    "                    print(out)\n",
    "\n",
    "            \n",
    "            # task4\n",
    "            if 'task4' in ans_path:\n",
    "                metrics = ['Acc','precision','recall']\n",
    "                # print('-'*50)\n",
    "                # print(out)\n",
    "                scores = hit_task4(out,tgt)\n",
    "\n",
    "\n",
    "            # task5 exam\n",
    "            if 'task5' in ans_path:\n",
    "                metrics = ['Acc']\n",
    "                # print('-'*50)\n",
    "                # print(out)\n",
    "                scores = task5_exam_acc(out,tgt)\n",
    "\n",
    "            # append to master dict, dataset object\n",
    "            scores_all[idx] = scores\n",
    "\n",
    "        # print(metrics)\n",
    "        print(model,np.mean(np.array(list(scores_all.values())),0))\n",
    "        avgs = np.mean(np.array(list(scores_all.values())),0,keepdims=False)\n",
    "        stds = np.std(np.array(list(scores_all.values())),0,keepdims=False)\n",
    "        if len(metrics) > 1:\n",
    "            for metric,avg,std in zip(metrics,avgs,stds):\n",
    "                metrics_list.append({'Task':'task{}'.format(task),'Model': model, 'Metric': metric, 'Average': avg, 'Standard Deviation': std})\n",
    "        else:\n",
    "            metrics_list.append({'Task':'task{}'.format(task),'Model': model, 'Metric': metrics[0], 'Average': avgs, 'Standard Deviation': stds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def summarize_metrics_from_files(directory, file_pattern=\"metrics.json\",metrics_list=None,nshot=None):\n",
    "    \n",
    "    all_metrics = pd.DataFrame(columns=['Task','Model','Metric', 'Average', 'Standard Deviation'])\n",
    "    if metrics_list is None:\n",
    "        metrics_list = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == file_pattern:\n",
    "                \n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                \n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "\n",
    "                task = os.path.basename(root)\n",
    "                \n",
    "                model_name = root.split('\\\\')[-2].split('_')[0]\n",
    "                \n",
    "                \n",
    "                for metric, scores in data.items():\n",
    "                    avg = scores['avg']\n",
    "                    std = scores['std']\n",
    "                    metrics_list.append({'Task':task,'Model': model_name, 'Metric': metric, 'Average': avg, 'Standard Deviation': std})\n",
    "    \n",
    "    \n",
    "    all_metrics = pd.concat([pd.DataFrame(metrics_list)], ignore_index=True)\n",
    "\n",
    "    \n",
    "    all_metrics.to_csv('summary_result_{}shot.csv'.format(nshot), index=False)\n",
    "    return all_metrics\n",
    "\n",
    "\n",
    "nshot = 1\n",
    "directory_path = \"./result/PsychClinical/{}shot\".format(nshot)\n",
    "summary_df = summarize_metrics_from_files(directory_path,metrics_list=metrics_list,nshot=nshot)\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded files\n",
    "table_df = pd.read_excel('./table_new_0shot.xlsx')\n",
    "summary_result_df = pd.read_csv('./summary_result_1shot.csv')\n",
    "\n",
    "# Display the first few rows of each dataframe to understand their structure\n",
    "table_df.head(), summary_result_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name_map = {\n",
    "    'task1':{'BLEU':0,'ROUGE-L':1,'BERT':2,'integrity':3,'Acc':4},\n",
    "    'task2':{'Acc':0},\n",
    "    'task3':{'BLEU':0,'ROUGE-L':1,'BERT':2,'Acc_main':3,'Acc_diff':4},\n",
    "    'task4':{'Acc':0,'recall':1,'precision':2},\n",
    "    'task5':{'BLEU':0,'ROUGE-L':1,'BERT':2}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Function to format the data as 'avg±std' and round to 2 decimal places\n",
    "def format_data(avg, std):\n",
    "    return f\"{avg:.2f}±{std:.2f}\"\n",
    "\n",
    "# Update the matching logic to handle the differences in task names\n",
    "def get_column_name(task, metric):\n",
    "    # Handle special case for integrity metric\n",
    "    name =  col_name_map[task][metric]\n",
    "    if name == 0:\n",
    "        name = task \n",
    "    else:\n",
    "        name = f\"{task}.{name}\"\n",
    "    # Handle other metrics by appending the metric name with a dot separator\n",
    "    return name\n",
    "\n",
    "# Iterate over each row in summary_result_df and update the corresponding cells in table_df\n",
    "for index, row in summary_result_df.iterrows():\n",
    "    model = row['Model']\n",
    "    task = row['Task']  # Convert task1 to Task1 for matching\n",
    "    metric = row['Metric']\n",
    "    avg = row['Average']\n",
    "    std = row['Standard Deviation']\n",
    "    \n",
    "    # Find the column name in table_df that matches the task and metric\n",
    "    try:\n",
    "        column_name = get_column_name(task, metric)\n",
    "    except:\n",
    "        continue\n",
    "    if column_name in table_df.columns:\n",
    "        # Find the row in table_df that matches the model\n",
    "        model_row = table_df[table_df['model'] == model].index\n",
    "        if not model_row.empty:\n",
    "            # Update the cell with the formatted data\n",
    "            table_df.at[model_row[0], column_name] = format_data(avg, std)\n",
    "\n",
    "\n",
    "# 保存DataFrame为Excel文件\n",
    "table_df.to_excel('./table_new_1shot.xlsx', index=False)\n",
    "\n",
    "# Display the updated table_df\n",
    "table_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d63dcc1e1b6d5f8326653fc6a95456dc2f085f5738ba83b256fa5f6addc83591"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
